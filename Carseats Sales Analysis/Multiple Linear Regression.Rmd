---
title: "Multiple Linear Regression"
author: "Shritama Sengupta (22MSD7032) "
date: "2023-06-02"
output: html_document
---


### Loading Dataset
```{r}

# Install and load the required packages
#install.packages("ISLR")
library(ISLR)

# Load the Carseats dataset
data(Carseats)
head(Carseats)
```

### (a) Fit a multiple regression model to predict Sales using Price, Urban, and US.
```{r}
# Encode categorical variables as numeric
Carseats$ShelveLoc <- as.numeric(factor(Carseats$ShelveLoc))
Carseats$Urban <- as.numeric(factor(Carseats$Urban))
Carseats$US <- as.numeric(factor(Carseats$US))
head(Carseats)
```

### (b) Provide an interpretation of each coefficient in the model.
```{r}
# Fit a multiple regression model
model <- lm(Sales ~ Price +  Urban + US, data = Carseats)

# Print the summary of the model
summary(model)
```
Interpretation of each coefficient in the model:

1. Intercept (11.864812): The intercept represents the estimated average sales value when all other factors (price, shelf location, urban area, and US location) are zero. In simpler terms, it gives us a baseline sales value.

2. Price (-0.054459 ): The coefficient for price tells us how much the sales are expected to change when the price of a product increases by one unit. A negative coefficient suggests that higher prices are associated with lower sales.

3. Urban (-0.021916): As the Urban is a qualitative variable the value -0.021916 represents that the that Sales tend to be lower in Urban areas compared to non-Urban areas.

4. US (1.200573): The coefficient for US location tells us the difference in sales between products sold in the US and non-US locations. A positive coefficient suggests that products sold in the US tend to have higher sales compared to non-US locations.

Remember that these interpretations are based on the specific dataset and should be considered in the context of the data and your knowledge of the subject matter.

### (c) Write out the model in equation form, being careful to handle the qualitative variables properly.

ANS: Sales = 11.864812 + Price(-0.054459) + Urban(-0.021916) + US(1.200573)

### (d) For which of the predictors can you reject the null hypothesis H0 : Î²j = 0? Write your own code for the same with analysis.
```{r}
# Extract the p-values for each coefficient
p_values <- summary(model)$coefficients[, 4]

# Print the p-values for each predictor
cat("P-values for each predictor:\n")
for (i in 2:length(p_values)) {
  cat(names(p_values)[i], ": ", p_values[i], "\n")
  
  p_values <- summary(model)$coefficients[, "Pr(>|t|)"]

significant_predictors <- names(p_values[p_values < 0.05])
cat("Significant predictors:", significant_predictors, "\n")
}

```

### (e) On the basis of your analysis made to the previous question, fit a smaller model that only uses the predictors for which there is evidence of association with the outcome.

```{r}

model2 <- lm(Sales ~ Price + US, data = Carseats)
summary(model2)

```

### (f) How well do the models in (a) and (e) fit the data? Analyse and give justification.
```{r}
metrics_Table = data.frame( Model = c("1st_Model","2nd_Model"),
                            R2 = c(summary(model)$r.squared, summary(model2)$r.squared),
                           Adj_R2 = c(summary(model)$adj.r.squared, summary(model2)$adj.r.squared
                           )
)
metrics_Table
```

Conclusion - Based on the R^2 and Adj R^2 values, which indicate the amount of variance in the response variable (Sales) explained by the predictors and account for the number of predictors in the model, we can conclude that the model with both significant and non-significant variables performs better for predicting Sales. Although the second model only includes the significant variables, it has slightly lower R^2 and Adj R^2 values compared to the first model, which includes non-significant variables. This suggests that the inclusion of non-significant variables contributes to a better prediction of Sales in this case.

### (g) Using the model from (e), obtain 95 % confidence intervals for the coefficient(s). Again write your own code for the confidence interval!


```{r}
coefficients <- coef(model2)
standard_errors <- sqrt(diag(vcov(model2)))

# Set the desired confidence level
confidence_level <- 0.95

# Calculate the critical value for the confidence interval
critical_value <- qt((1 - confidence_level) / 2, df = df.residual(model2))

# Calculate the confidence intervals
lower_bounds <- coefficients - critical_value * standard_errors
upper_bounds <- coefficients + critical_value * standard_errors

# Combine the lower and upper bounds
confidence_intervals <- cbind(lower_bounds, upper_bounds)

# Print the confidence intervals
print(confidence_intervals)
```

